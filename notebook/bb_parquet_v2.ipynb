{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking Fraud with various Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data](#Data)\n",
    "4. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "    1. [Investigating the distribution of fraudulent values](#Investigating-the-distribution-of-fraudulent-values)\n",
    "    2. [Investigating transaction flows](#Investigating-transaction-flows)\n",
    "    3. [Investigating account names](#Investigating-account-names)\n",
    "    4. [Investigating transactions](#Investigating-transactions)\n",
    "    5. [Investigating time](#Investigating-time)\n",
    "5. [Feature Engineering](#Feature-Engineering)\n",
    "   1. [Removing columns and specific types](#Removing-columns-and-specific-types)\n",
    "   2. [Adding the **hourOfDay** feature](#Adding-the-hourOfDay-feature)\n",
    "   3. [Changing categorical values](#Changing-categorical-values)\n",
    "   4. [Splitting and standardising](#Splitting-and-standardising)\n",
    "6. [Models](#Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lack of public available datasets on financial services and specially in the emerging mobile money transactions domain. Financial datasets are important to many researchers and in particular to us performing research in the domain of fraud detection. Part of the problem is the intrinsically private nature of financial transactions, that leads to no publicly available datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presented here is a synthetic dataset generated using the simulator called PaySim as an approach to such a problem. PaySim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import AWS specific modules and specify S3 data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyarrow\n",
    "# ! pip install s3fs\n",
    "# ! pip install seaborn\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade seaborn\n",
    "import boto3\n",
    "from random import seed, sample\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import io\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python ML modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, precision_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket='sagemaker-pmelvin'\n",
    "parquet_data = 'input-parquet-single/part-00000-28137ef0-c04d-436a-8b20-ae663dbe740b-c000.snappy.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and perform initial analysis (parquet from S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "buffer = io.BytesIO()\n",
    "client = boto3.resource('s3')\n",
    "\n",
    "object = client.Object(bucket, parquet_data)\n",
    "object.download_fileobj(buffer)\n",
    "\n",
    "# df = pd.read_parquet(buffer)\n",
    "df = pq.read_table(buffer).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'nameorig':'nameOrig','oldbalanceorg':'oldBalanceOrig', \\\n",
    "                        'newbalanceorig':'newBalanceOrig', 'namedest':'nameDest', \\\n",
    "                        'oldbalancedest':'oldBalanceDest', 'newbalancedest':'newBalanceDest', \\\n",
    "                        'isfraud':'isFraud', 'isflaggedfraud':'isFlaggedFraud'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, the dataset has roughly 6.5 million records, where each record uses 11 attributes (features) to describe the profile of the transactions. The attributes are:\n",
    "- `step`: maps a unit of time in the real world. In this case 1 step is 1 hour of time\n",
    "- `type`: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER\n",
    "- `amount`: the amount of the transaction in local currency\n",
    "- `nameOrig`: the customer who started the transaction\n",
    "- `oldBalanceOrig`: the initial balance before the transaction\n",
    "- `newBalanceOrig`: the customer's balance after the transaction\n",
    "- `nameDest`: the recipient ID of the transaction\n",
    "- `oldBalanceDest`: the initial recipient balance before the transaction\n",
    "- `newBalanceDest`: the recipient's balance after the transaction\n",
    "- `isFraud`: this identifies a fraudulent transaction (1) and non fraudulent (0)\n",
    "- `isFlaggedFraud`: this identifies a flagged fraudulent transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check to see if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the distribution of fraudulent values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know there are five transaction types, lets see how the are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.type.value_counts())\n",
    "print()\n",
    "\n",
    "fig = plt.subplots(figsize=(6,4))\n",
    "ax = df.type.value_counts().plot(kind='bar', title=\"Transaction type\")\n",
    "ax.set_ylabel(\"Number of transactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also know that there are two features that look interesting **isFraud** and **isFlaggedFraud** and from the description _isFraud_ indicates actual fraudulent transactions whereas _isFlaggedFraud_ is when the system prevents the transaction due to some condition\n",
    "\n",
    "Lets see how many transactions are fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(6,4))\n",
    "sns.countplot(x='type', hue='isFraud', data=df, order = df['type'].value_counts().index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its clear there are some fraudulent transactions but it is very difficult to see how many as in comparison to the entire dataset the amount is tiny\n",
    "\n",
    "Therefore, lets find out how many are actual fraud and in what type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(6,4))\n",
    "ax = df.groupby(['type', 'isFraud']).size().plot(kind='bar')\n",
    "ax.set_xlabel(\"(type, isFraud)\")\n",
    "ax.set_ylabel(\"Number of transactions\")\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it looks like that only **CASH_OUT** and **TRANSFER** have actual fraudulent transactions\n",
    "\n",
    "Now lets have a look at which transactions are being flagged as fraudulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(6,4))\n",
    "sns.countplot(x='type', hue='isFlaggedFraud', data=df, order = df['type'].value_counts().index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike **isFraud** we cannot even see which are flagged as fraudulent so lets find out which type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(6,4))\n",
    "ax = df.groupby(['type', 'isFlaggedFraud']).size().plot(kind='bar')\n",
    "ax.set_xlabel(\"(type, isFlaggedFraud)\")\n",
    "ax.set_ylabel(\"Number of transactions\")\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot it looks like that only **TRANSFER** has any flagged transactions and the amount is only 16 records out of 6.5 million!\n",
    "\n",
    "Due to the insignificant percentage we can also remove th **isFlaggedFraud** feature as well\n",
    "\n",
    "So we now know that we can focus on **CASH_OUT** and **TRANSFER** and remove the other types, now let's have a look at the source and direction of the transcactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating transaction flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a recap let's look at some of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on the **nameOrig** and **nameDest** and see if we can see a pattern\n",
    "\n",
    "The format for the value has either a **C** or **M** before some digits. However, the letters are not defined, and it would be reasonable to assume that **C** stands for **Customer** and **M** stands for **Merchant**, let's have a look at the relationships between transaction flows between **C** and **M**\n",
    "\n",
    "We will create a new feature called _direction_ which will correlate to the following:\n",
    "1. CC - Customer to customer\n",
    "2. CM - Customer to merchant\n",
    "3. MC - Merchant to customer\n",
    "4. MM - Merchant to merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_direction = df.copy()\n",
    "df_direction['direction'] = np.nan\n",
    "\n",
    "df_direction.loc[df.nameOrig.str.contains('C') & df.nameDest.str.contains('C'),\"direction\"] = \"CC\" \n",
    "df_direction.loc[df.nameOrig.str.contains('C') & df.nameDest.str.contains('M'),\"direction\"] = \"CM\"\n",
    "df_direction.loc[df.nameOrig.str.contains('M') & df.nameDest.str.contains('C'),\"direction\"] = \"MC\"\n",
    "df_direction.loc[df.nameOrig.str.contains('M') & df.nameDest.str.contains('M'),\"direction\"] = \"MM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split up the fraudulent and non-fraudlent transactions so we can refer to these later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = df_direction[df_direction[\"isFraud\"] == 1]\n",
    "valid = df_direction[df_direction[\"isFraud\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraud transactions by direction:\\n\",fraud.direction.value_counts())\n",
    "print()\n",
    "print(\"Valid transactions by direction:\\n\",valid.direction.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it looks like the only fraudulent transactions occur between customer accounts (CC) so like the type above we can remove the extraneous information when we do feature engineering\n",
    "\n",
    "Using the new table we can confirm that **CASH_OUT** and **TRANSFER** are the only types with fraudulent activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating account names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to figure out if the originator and the destination are the same relating to removing the money using the **CASH_OUT**\n",
    "\n",
    "First we start by confirming that only the **CASH_OUT** and **TRANSFER** have the fraudulent activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_in=df.loc[(df.isFraud==1) & (df.type=='CASH_IN')]\n",
    "cash_out=df.loc[(df.isFraud==1) & (df.type=='CASH_OUT')]\n",
    "debit=df.loc[(df.isFraud==1) & (df.type=='DEBIT')]\n",
    "payment=df.loc[(df.isFraud==1) & (df.type=='PAYMENT')]\n",
    "transfer=df.loc[(df.isFraud==1) & (df.type=='TRANSFER')]\n",
    "\n",
    "print('Fraudulent transacions in CASH_IN',len(cash_in))\n",
    "print('Fraudulent transacions in CASH_OUT',len(cash_out))\n",
    "print('Fraudulent transacions in DEBIT',len(debit))\n",
    "print('Fraudulent transacions in PAYMENT',len(payment))\n",
    "print('Fraudulent transacionsin TRANSFER',len(transfer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which they are, so that is good :) (well not really!)\n",
    "\n",
    "Now we need to test whether the account to _cashout_ is actually used to remove the money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_transfer = fraud[fraud[\"type\"] == \"TRANSFER\"]\n",
    "fraud_cashout = fraud[fraud[\"type\"] == \"CASH_OUT\"]\n",
    "\n",
    "fraud_transfer.nameDest.isin(fraud_cashout.nameOrig).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns false, so we now know that the account used to remove the fund was not the same as the one that received the funds\n",
    "\n",
    "Therefore, we can remove the **nameOrig** and **nameDest** as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the amounts of fraudulent vs legitimate transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('float_format', '{:,.2f}'.format)\n",
    "print(\"Description of amounts moved in fraudulent transactions: \\n\",pd.DataFrame.describe(fraud.amount),\"\\n\")\n",
    "print(\"Description of amounts moved moved in legitimate transactions: \\n\", pd.DataFrame.describe(valid.amount),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can clearly see that the maximum amount for a fraudulent transaction is 10 million, with the average of 1.5 million, however there is no real limit for legimate transactions (maximum of 100 million) but the average is much lower, around 200 thousand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets have a look at the balances before and after the transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_originating_bal = sum(df[\"oldBalanceOrig\"] - df[\"amount\"] != df[\"newBalanceOrig\"])\n",
    "wrong_destination_bal = sum(df[\"newBalanceDest\"] + df[\"amount\"] != df[\"newBalanceDest\"])\n",
    "\n",
    "print(\"% of transactions with balance errors in the originating account: \", 100*round(wrong_originating_bal/len(df),2))\n",
    "print(\"% of transactions with balance errors in the destination account: \", 100*round(wrong_destination_bal/len(df),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like almost all of the balance/transactions have errors, so let's have a closer look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "amount_given = sum(df[\"amount\"] > df[\"oldBalanceOrig\"])\n",
    "amount_given = \"{:,}\".format(amount_given)\n",
    "print(\"The number of occurances where the amount given is greater than the amount that is in the originator's account: \", amount_given)\n",
    "\n",
    "amount_received = sum(df[\"amount\"] > df[\"newBalanceDest\"])\n",
    "amount_received = \"{:,}\".format(amount_received)\n",
    "print(\"The number of occurances where the amount received is greater than the amount that is in the receiver's account: \", amount_received)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above we can confirm that there are some fundamental erros in the datset as normally it would be impossible to give/receive more than than is availble in your account, therefore we cannot ignore this values but simply be aware of them (and the errors within)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final look at the dataset, we will look at the time or **step**\n",
    "\n",
    "We know that the **step** relates to 1 hour over a period of thrity days and we need to have a look if there is any correlation to fraudulent transactions over days or hours\n",
    "\n",
    "Let's start with having a look over the entire month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(valid, x='step', bins=30, color='seagreen')\n",
    "plt.xlabel(\"step - 1 hour\")\n",
    "plt.title(\"# of valid transactions over a month\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(fraud, x='step', bins=30, color='indianred')\n",
    "plt.xlabel(\"step - 1 hour\")\n",
    "plt.title(\"# of fraudulent transactions over a month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above:\n",
    "- for valid transactions, the majority occurs between steps 0 - 60 and 125 - 400\n",
    "- for fraudulent transactions, there is no real pattern, they occur at all times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look over a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 7\n",
    "\n",
    "fraud_days = fraud.step % days\n",
    "valid_days = valid.step % days\n",
    "\n",
    "sns.histplot(valid_days, bins=days, color='seagreen')\n",
    "plt.xlabel(\"step - 1 day\")\n",
    "plt.title(\"# of valid transactions over a week\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(fraud_days, bins=days, color='indianred')\n",
    "plt.xlabel(\"step - 1 day\")\n",
    "plt.title(\"# of fraudulent transactions over a week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above there is no indication that fraudulent transactions happen on a particular day, they occur every day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = 24\n",
    "\n",
    "fraud_hours = fraud.step % hours\n",
    "valid_hours = valid.step % hours\n",
    "\n",
    "sns.histplot(valid_hours, bins=hours, color='seagreen')\n",
    "plt.xlabel(\"step - 1 hour\")\n",
    "plt.title(\"# of valid transactions over a day\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(fraud_hours, bins=hours, color='indianred')\n",
    "plt.xlabel(\"step - 1 hour\")\n",
    "plt.title(\"# of fraudulent transactions over a day\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what can we infer from the above?\n",
    "\n",
    "It looks like that fraudulent transactions occur all hours of the day but valid transactions do not seem to occur in the steps between 0 - 9\n",
    "\n",
    "Therefore, it is a good idea to create a new feature based on hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hourOfDay'] = np.nan\n",
    "df.hourOfDay = df_direction.step % 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's have a recap of which features we can remove from our investigations\n",
    "\n",
    "From the **type** feature we are only interested in **CASH_OUT** and **TRANSFER**\n",
    "\n",
    "We also don't need to include **nameOrig** and **nameDest**\n",
    "\n",
    "**isFlaggedFraud** is neglible so we can also remove that (remember there were only sixteen values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing columns and specific types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can remove these features using pandas, it is worth using other tools (Glue and Athena) to reduce th dataset and then reload and continue with the feature engineering process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/glue_transform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and I need to add some code to combine the standard frames into one **dynamic_Frame=applymapping1.coalesce(1)**\n",
    "\n",
    "once I have run another crawler, I use Athena to remove the specific types to only leave **CASH_OUT** and **TRANSFER**\n",
    "\n",
    "SELECT *\\\n",
    "FROM removed_columns\\\n",
    "WHERE type = 'CASH_OUT' or type = 'TRANSFER';\n",
    "\n",
    "the new file is then saved to S3 which i can then use directly (as it is a CSV) or convert it parquet using Glue (as before)\n",
    "\n",
    "So some information on the origanl file and the feature engineered (well basic cleaning so far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original CSV file size    : 470.7MB\\\n",
    "Origanal Parquet file size: 274.8MB\n",
    "\n",
    "Columns removed CSV file size    : 319.2MB\\\n",
    "Columns removed Parquet file size: 158.1MB\n",
    "\n",
    "Feature **type** cleaned CSV file size    : 187.1MB\\\n",
    "Feature **type** cleaned Parquet file size: 80.1MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_original      = 'input-data/bb_banking_fraud.csv'\n",
    "csv_removed_cols  = 'input-data/removed_columns/run-1606989332572-part-r-00000'\n",
    "csv_removed_types = 'input-data/removed-specific-types/01a89090-e9b6-4b07-8a49-b0244d6dc035.csv'\n",
    "\n",
    "par_original      = 'input-parquet-single/part-00000-28137ef0-c04d-436a-8b20-ae663dbe740b-c000.snappy.parquet'\n",
    "par_removed_cols  = 'input-parquet-single/removed-columns/part-00000-c88815d4-0920-4ab0-bb77-b160ab9dbc09-c000.snappy.parquet'\n",
    "par_removed_types = 'input-parquet-single/removed-specific-types/part-00000-c2ad1434-15f7-4fb0-afb2-54757f53e21b-c000.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local\n",
    "csv_original      = 'bb_banking_fraud.csv'\n",
    "csv_removed_cols  = 'run-1606989332572-part-r-00000'\n",
    "csv_removed_types = '01a89090-e9b6-4b07-8a49-b0244d6dc035.csv'\n",
    "\n",
    "# par_original      = 'part-00000-28137ef0-c04d-436a-8b20-ae663dbe740b-c000.snappy.parquet'\n",
    "# par_removed_cols  = 'part-00000-c88815d4-0920-4ab0-bb77-b160ab9dbc09-c000.snappy.parquet'\n",
    "# par_removed_types = 'part-00000-c2ad1434-15f7-4fb0-afb2-54757f53e21b-c000.snappy.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and quickly double-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_csv_original = pd.read_csv(csv_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_csv_removed_cols = pd.read_csv(csv_removed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_csv_removed_types = pd.read_csv(csv_removed_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_original.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_removed_cols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_removed_cols.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_removed_types.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_removed_types.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in summary, so far, for the CSV files the load time comes down by a factor of about 10 which is great!\n",
    "\n",
    "Let's fix those column names again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_removed_types = df_csv_removed_types.rename(columns={'oldbalanceorg':'oldBalanceOrig', 'newbalanceorig':'newBalanceOrig', 'oldbalanceoest':'oldBalanceDest', 'newbalancedest':'newBalanceDest', 'isfraud':'isFraud'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_csv_removed_types\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the **hourOfDay** feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "df['hourOfDay'] = np.nan\n",
    "df.hourOfDay = df_copy.step % 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can also perform the same action in Athena\n",
    "\n",
    "SELECT *,  MOD(step, 24) timeofday\\\n",
    "from removed_columns \n",
    "\n",
    "which returns a CSV file with a new column **timeofday** which is _step % 24_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_modulus       = 'a7aa763d-235b-45d1-bed6-3364ead885bc.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Changing categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the machine learning algorithms can not handle categorical variables unless we convert them to numerical values. Many algorithm’s performances vary based on how Categorical variables are encoded.\n",
    "\n",
    "Categorical variables can be divided into two categories: Nominal (No particular order) and Ordinal (some ordered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no ordering we will use **One-Hot encoding**.\\\n",
    "In this method, we map each category to a vector that contains 1 and 0 denoting the presence or absence of the feature. The number of vectors depends on the number of categories for features. This method produces a lot of columns that slows down the learning significantly if the number of the category is very high for the feature.\n",
    "\n",
    "pandas has the **get_dummies** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummies = pd.get_dummies(df, prefix=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummies.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Splitting and standardising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train any machine learning model irrespective what type of dataset is being used you have to **split** the dataset into training data and testing data.\n",
    "\n",
    "scikit has **train_test_split** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to allow reproducible splitting we define a RandomState and seed\n",
    "randomstate = 25\n",
    "seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_with_dummies.drop(\"isFraud\",1)\n",
    "y = df_with_dummies.isFraud\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=randomstate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The X_train shape is {X_train.shape} and the y_train shape is {y_train.shape}\")\n",
    "print(f\"The X_test shape is {X_test.shape} and the y_test shape is {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above splits the data but is it the best option?\n",
    "\n",
    "Possible not, as the split will always have the same data and this can lead to what is described as the **bias-variance-tradeoff**\n",
    "\n",
    "To combat this we can use **cross-validation**, which is a method of model validation which splits the data in creative ways in order to obtain the better estimates of “real world” model performance, and minimise validation error.\n",
    "\n",
    "**K-fold** validation is a popular method of cross validation which shuffles the data and splits it into k number of folds (groups). In general K-fold validation is performed by taking one group as the test data set, and the other k-1 groups as the training data, fitting and evaluating a model, and recording the chosen score. This process is then repeated with each fold (group) as the test data and all the scores averaged to obtain a more comprehensive model validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we mean by **standardisation**?\n",
    "\n",
    "**Standardise** generally means changing the values so that the distribution standard deviation from the mean equals one. It outputs something very close to a normal distribution. Scaling is often implied.\n",
    "\n",
    "**Scaling** generally means to change the range of the values. The shape of the distribution doesn’t change. Think about how a scale model of a building has the same proportions as the original, just smaller. That’s why we say it is drawn to scale. The range is often set at 0 to 1.\n",
    "\n",
    "Many machine learning algorithms perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed. **Scaling** and **standardising** can help features arrive in more digestible form for these algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}